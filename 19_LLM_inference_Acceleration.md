# LLM inference Acceleration
https://zhuanlan.zhihu.com/p/655325832
## KV Cache
https://baijiahao.baidu.com/s?id=1763480207227229622&wfr=spider&for=pc

## Page attention and Flash attention
https://zhuanlan.zhihu.com/p/642802585
http://lihuaxi.xjx100.cn/news/1335720.html?action=onClick
https://baijiahao.baidu.com/s?id=1774803715921029316&wfr=spider&for=pc
https://zhuanlan.zhihu.com/p/645376942
## **multi-query attention (MQA) and grouped query attention (GQA)**

https://zhuanlan.zhihu.com/p/647130255
